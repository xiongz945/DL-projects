{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2P2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQDTA_B0-hsF"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd, nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fURjEoQT_zZw"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, use_conv3=False, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        if use_conv3:\n",
        "          self.shortcut = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride, bias=False)\n",
        "        else:\n",
        "          self.shortcut = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.shortcut:\n",
        "          x = self.shortcut(x)\n",
        "        out = F.relu(out+x)\n",
        "        return out\n",
        "\n",
        "def resnet_block(in_channel, out_channel, num_residuals, first_block=False,stride=1): \n",
        "  blk = nn.Sequential() \n",
        "  for i in range(num_residuals): \n",
        "    block_name = 'block{}'.format(i+1)\n",
        "    if i == 0:\n",
        "      if not first_block: \n",
        "        blk.add_module(block_name,ResidualBlock(in_channel, out_channel, use_conv3=True, stride=stride))\n",
        "      else:\n",
        "        blk.add_module(block_name,ResidualBlock(in_channel, out_channel, use_conv3=False, stride=stride))\n",
        "    else: \n",
        "      blk.add_module(block_name,ResidualBlock(out_channel, out_channel, stride=1)) \n",
        "  return blk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54sOF5KE0ie"
      },
      "source": [
        "def init_weights(module):\n",
        "  if isinstance(module, nn.Linear):\n",
        "    nn.init.xavier_normal_(module.weight.data)\n",
        "  if isinstance(module, nn.Conv2d):\n",
        "    nn.init.kaiming_normal_(module.weight.data, mode='fan_out')\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "  def __init__(self, batch_size):\n",
        "    super(Resnet, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, 3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU()\n",
        "        # nn.MaxPool2d(3, stride=2, padding=1)\n",
        "    )\n",
        "    self.stage1 = resnet_block(64,64,2,first_block=True,stride=1)\n",
        "    self.stage2 = resnet_block(64,128,2,stride=2)\n",
        "    self.stage3 = resnet_block(128,256,2,stride=2)\n",
        "    # self.stage4 = resnet_block(64,128,2,stride=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.conv_output_size = self._forward_sub(torch.zeros(batch_size,3,64,64)).view(batch_size, -1).shape[1]\n",
        "    print(\"conv output size is \",self.conv_output_size)\n",
        "    self.fc = nn.Linear(self.conv_output_size, 4000, bias=False)\n",
        "\n",
        "    self.apply(init_weights)\n",
        "  \n",
        "  def _forward_sub(self, x):\n",
        "    x = self.layers(x)\n",
        "    x = self.stage1(x)\n",
        "    x = self.stage2(x)\n",
        "    x = self.stage3(x)\n",
        "    x = F.adaptive_avg_pool2d(x, output_size=1)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self._forward_sub(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ3ernml39fW"
      },
      "source": [
        "def xavier_init(params):\n",
        "  for m in params:\n",
        "    if isinstance(m,nn.Linear) or isinstance(m,nn.Conv2d):\n",
        "      nn.init.xavier_normal_(m.weight)\n",
        "      \n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "\n",
        "    self.conv_1 = nn.Conv2d(3,64,3,1,1)\n",
        "    self.bn_1 = nn.BatchNorm2d(64)\n",
        "    # self.do_1 = nn.Dropout2d(0.3)\n",
        "    self.conv_2 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.bn_2 = nn.BatchNorm2d(64)\n",
        "    self.pool_1 = nn.MaxPool2d(2,2)\n",
        "    self.conv_3 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.bn_3 = nn.BatchNorm2d(64)\n",
        "    self.pool_2 = nn.MaxPool2d(2,2)\n",
        "    self.conv_4 = nn.Conv2d(64,128,3,1,1)\n",
        "    self.bn_4 = nn.BatchNorm2d(128)\n",
        "    self.do_2 = nn.Dropout2d(0.3)\n",
        "    self.conv_5 = nn.Conv2d(128,128,3,1,1)\n",
        "    self.bn_5 = nn.BatchNorm2d(128)\n",
        "    self.pool_3 = nn.MaxPool2d(2,2)\n",
        "    self.conv_6 = nn.Conv2d(128,256,3,1,1)\n",
        "    self.bn_6 = nn.BatchNorm2d(256)\n",
        "    self.do_3 = nn.Dropout2d(0.3)\n",
        "    self.conv_7 = nn.Conv2d(256,256,3,1,1)\n",
        "    self.bn_7 = nn.BatchNorm2d(256)\n",
        "    self.pool_4 = nn.MaxPool2d(2,2)\n",
        "    self.conv_8 = nn.Conv2d(256,256,3,1,1)\n",
        "    self.bn_8 = nn.BatchNorm2d(256)\n",
        "    self.pool_5 = nn.MaxPool2d(2,2)\n",
        "    self.conv_9 = nn.Conv2d(256,256,3,1,1)\n",
        "    self.bn_9 = nn.BatchNorm2d(256)\n",
        "    self.pool_6 = nn.MaxPool2d(2,2)\n",
        "    self.fc = nn.Linear(256,4000,bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.do_1(F.relu(self.bn_1(self.conv_1(x))))\n",
        "    x = F.relu(self.bn_1(self.conv_1(x)))\n",
        "    x = F.relu(self.bn_2(self.conv_2(x)))\n",
        "    x = self.pool_1(x)\n",
        "    x = F.relu(self.bn_3(self.conv_3(x)))\n",
        "    x = self.pool_2(x)\n",
        "    x = self.do_2(F.relu(self.bn_4(self.conv_4(x))))\n",
        "    x = F.relu(self.bn_5(self.conv_5(x)))\n",
        "    x = self.pool_3(x)\n",
        "    x = self.do_3(F.relu(self.bn_6(self.conv_6(x))))\n",
        "    x = F.relu(self.bn_7(self.conv_7(x)))\n",
        "    x = self.pool_4(x)\n",
        "    x = F.relu(self.bn_8(self.conv_8(x)))\n",
        "    x = self.pool_5(x)\n",
        "    x = F.relu(self.bn_9(self.conv_9(x)))\n",
        "    x = self.pool_6(x)\n",
        "    # print(x.shape)\n",
        "    x = x.view(-1,256)\n",
        "    # print(x.shape)\n",
        "    x = self.fc(x)    \n",
        "    return x\n",
        "  \n",
        "  def init_weights(self):\n",
        "    with torch.no_grad():\n",
        "      xavier_init(self.modules())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m4hLVUUFVSr"
      },
      "source": [
        "import time\n",
        "def train(model, data_loader, test_loader, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      start_time = time.time()\n",
        "      avg_loss = 0.0\n",
        "      for batch_num, (features, labels) in enumerate(data_loader):\n",
        "          features, labels = features.to(device), labels.to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(features)\n",
        "          loss = criterion(outputs, labels.long())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          avg_loss += loss.item()\n",
        "\n",
        "          if batch_num % 400 == 399:\n",
        "              print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/400))\n",
        "              end_time = time.time()\n",
        "              print(f\"400 batches took {end_time - start_time} seconds\")\n",
        "              avg_loss = 0.0    \n",
        "              start_time = time.time()\n",
        "          \n",
        "          torch.cuda.empty_cache()\n",
        "          del features\n",
        "          del labels\n",
        "          del loss\n",
        "      scheduler.step()\n",
        "      if task == 'Classification':\n",
        "          val_loss, val_acc = test_classify(model, test_loader)\n",
        "          train_loss, train_acc = test_classify(model, data_loader)\n",
        "          print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "                format(train_loss, train_acc, val_loss, val_acc))\n",
        "      else:\n",
        "          test_verify(model, test_loader)\n",
        "      \n",
        "      \n",
        "      torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict' : scheduler.state_dict(),\n",
        "      }, \"/content/\"+\"Model_\"+str(epoch))\n",
        "\n",
        "def test_classify(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = []\n",
        "  accuracy = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_num, (features, labels) in enumerate(test_loader):\n",
        "      features, labels = features.to(device), labels.to(device)\n",
        "      outputs = model(features)\n",
        "            \n",
        "      _, pred_labels = torch.max(outputs,1)\n",
        "      # pred_labels = pred_labels.view(-1)\n",
        "      \n",
        "      loss = criterion(outputs, labels.long())\n",
        "      \n",
        "      accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "      total += len(labels)\n",
        "      test_loss.extend([loss.item()]*features.size()[0])\n",
        "      del features\n",
        "      del labels\n",
        "\n",
        "  model.train()\n",
        "  return np.mean(test_loss), accuracy/total\n",
        "\n",
        "def test_verify(model, test_loader):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zgRfOG8eFE6"
      },
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/classification_data/train_data',transform=torchvision.transforms.ToTensor())\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
        "val_dataset = torchvision.datasets.ImageFolder(root='/content/classification_data/val_data',transform=torchvision.transforms.ToTensor())\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='/content/classification_data/test_data',transform=torchvision.transforms.ToTensor())\n",
        "test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6w9nUASNQCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d92a931-c4b0-4752-fff7-0a28e3c85bd7"
      },
      "source": [
        "print(train_dataset.__len__(), len(train_dataset.classes))\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "380638 4000\n",
            "torch.Size([256, 3, 64, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNLZewqAav7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95813c0c-653a-4e39-9f67-6057ce5d8e20"
      },
      "source": [
        "# Resnet\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "net = MyModel()\n",
        "print(net)\n",
        "net.to(device)\n",
        "epochs = 20\n",
        "learningRate = 0.15\n",
        "weightDecay = 5e-5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.1,verbose=True, patience=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "MyModel(\n",
            "  (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (do_2): Dropout2d(p=0.3, inplace=False)\n",
            "  (conv_5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (do_3): Dropout2d(p=0.3, inplace=False)\n",
            "  (conv_7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv_9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool_6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=256, out_features=4000, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju1u776Q8Lwl",
        "outputId": "e0f94588-d475-4dc8-ee46-e0741674573e"
      },
      "source": [
        "pytorch_total_params = sum([p.numel() for n, p in net.named_parameters() if \"weight\" in n])\n",
        "print(pytorch_total_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3386496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHS4uI785P3H"
      },
      "source": [
        "epochs = 5\n",
        "temp = torch.load(\"Model_14\")\n",
        "net.load_state_dict(temp['model_state_dict'])\n",
        "optimizer.load_state_dict(temp['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(temp['scheduler_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmVDxdW5Mv7I"
      },
      "source": [
        "# Baseline\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "# net = MyModel()\n",
        "# net.init_weights()\n",
        "# print(net)\n",
        "# net.to(device)\n",
        "# epochs = 10\n",
        "# learningRate = 0.15\n",
        "# weightDecay = 5e-5\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw-BQBXIs14F"
      },
      "source": [
        "train(net, train_dataloader, test_dataloader) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUDYSQq8gkRm"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class VerifyDataset(Dataset):\n",
        "  def __init__(self, dir):\n",
        "    data_file = open(dir)\n",
        "    self.file_list = []\n",
        "    lines = data_file.readlines()\n",
        "    for line in lines:\n",
        "      img1, img2 = line.strip('\\n').split(' ')\n",
        "      self.file_list.append((img1, img2))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img1_dir, img2_dir = self.file_list[index]\n",
        "    img1 = Image.open(img1_dir)\n",
        "    img2 = Image.open(img2_dir)\n",
        "    img1 = torchvision.transforms.ToTensor()(img1)\n",
        "    img2 = torchvision.transforms.ToTensor()(img2)\n",
        "    return img1, img2\n",
        "\n",
        "veri_dataset = VerifyDataset(\"/content/verification_pairs_test.txt\")\n",
        "veri_dataloader = torch.utils.data.DataLoader(veri_dataset, batch_size=10, shuffle=False, num_workers=1)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "def verify(model, dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  for i, (img1, img2) in enumerate(dataloader):\n",
        "    img1 = img1.to(device)\n",
        "    img2 = img2.to(device)\n",
        "\n",
        "    emb_1 = model(img1)\n",
        "    emb_2 = model(img2)\n",
        "\n",
        "    similarity = cos(emb_1, emb_2)\n",
        "    similarity.view(-1, 1)\n",
        "    res.append(similarity.cpu().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLAkke6C6EAO"
      },
      "source": [
        "import copy\n",
        "verify_net = copy.deepcopy(net)\n",
        "verify_net.fc = nn.Sequential()\n",
        "res = []\n",
        "verify(verify_net, veri_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux01QfJb9sRO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bfd7383-1073-4d4c-8369-9d9eff10d674"
      },
      "source": [
        "ans = sum(res, [])\n",
        "print(len(ans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCsZ2lKn_jKT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6a4fbf9-4835-48b9-f0dd-df8feaf6244c"
      },
      "source": [
        "id = [file1+' '+file2 for (file1, file2) in veri_dataset.file_list]\n",
        "print(id[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "verification_data/00020839.jpg verification_data/00035322.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuQpCJbJAlML"
      },
      "source": [
        "import pandas as pd\n",
        "d = {\"Id\":id, \"Category\":ans}\n",
        "df = pd.DataFrame(d)\n",
        "df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}