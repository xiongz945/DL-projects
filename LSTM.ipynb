{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3P2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsSdDzcBJNUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fdb9f4-ebd5-4ec7-8714-cc3b19f33d25"
      },
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 1063 (delta 6), reused 7 (delta 2), pack-reused 1047\u001b[K\n",
            "Receiving objects: 100% (1063/1063), 763.61 KiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (509/509), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 90, done.        \n",
            "remote: Counting objects: 100% (90/90), done.        \n",
            "remote: Compressing objects: 100% (64/64), done.        \n",
            "remote: Total 13672 (delta 41), reused 54 (delta 21), pack-reused 13582        \n",
            "Receiving objects: 100% (13672/13672), 5.53 MiB | 21.07 MiB/s, done.\n",
            "Resolving deltas: 100% (7847/7847), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=88cce8acce8408bc434d8fcc45cf9514bac771cbd2d3998eb654baa9c6456141\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "/content/ctcdecode\n",
            "Processing /content/ctcdecode\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.2-cp36-cp36m-linux_x86_64.whl size=12754073 sha256=7828158c47c5a63cd04c51579baa15ddf7aff67f08135973617a47b7bce2b0d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jiz973l4/wheels/c3/6c/94/7d57d4f20a87a22ef1722eaad22052b4c435892b55400e5f4e\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.2\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLCeMyQ5Zf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53987a3c-57bd-43f2-f1cb-537a224c57d2"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 28.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144794 sha256=863a4c8fa485932728fb3abc085ba2ccf51886262ad8e002936b0f1e5695c06a\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn9dbB0CL-5p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.tensor as tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import *\n",
        "import time\n",
        "import Levenshtein as ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqm2zGr5e6DE"
      },
      "source": [
        "batch_size = 64\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "num_of_workers = 4 if cuda else 0\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTNeao-OmCnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe6e41f-18e2-402c-f0bd-ad89dcd32f3e"
      },
      "source": [
        "from phoneme_list import N_PHONEMES, PHONEME_LIST, PHONEME_MAP\n",
        "print(len(PHONEME_MAP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIUleLaEjED_"
      },
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "import os\n",
        "decoder = CTCBeamDecoder(PHONEME_MAP, beam_width=25, num_processes=os.cpu_count(), log_probs_input=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcCsaJGQYt4j"
      },
      "source": [
        "train_features = np.load('train.npy', allow_pickle=True, encoding='latin1')\n",
        "train_labels = np.load('train_labels.npy', allow_pickle=True, encoding='latin1')\n",
        "val_features = np.load('dev.npy', allow_pickle=True, encoding='latin1')\n",
        "val_labels = np.load('dev_labels.npy', allow_pickle=True, encoding='latin1')\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    X = self.X[index]\n",
        "    X_len = X.shape[0]\n",
        "    Y = self.Y[index] + 1\n",
        "    Y_len = Y.shape[0]\n",
        "    return torch.from_numpy(X).float(), X_len, torch.from_numpy(Y).float(), Y_len\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  X, X_len, Y, Y_len = zip(*batch)\n",
        "  X_lens = torch.LongTensor(X_len)\n",
        "  Y_lens = torch.LongTensor(Y_len)\n",
        "  X_pad = pad_sequence(X)\n",
        "  Y_pad = pad_sequence(Y, batch_first=True)\n",
        "  return X_pad, X_lens, Y_pad, Y_lens\n",
        "\n",
        "train_dataset = MyDataset(train_features, train_labels)\n",
        "val_dataset = MyDataset(val_features, val_labels)\n",
        "train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_of_workers, pin_memory=True, collate_fn=collate_fn)\n",
        "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, num_workers=num_of_workers, collate_fn=collate_fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8CA6Ut7ae6G"
      },
      "source": [
        "def xavier_init(params):\n",
        "  for m in params:\n",
        "    if isinstance(m, nn.Linear):\n",
        "      nn.init.xavier_normal_(m.weight)\n",
        "      nn.init.zeros_(m.bias)\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, in_utter, out_phone, hidden_size):\n",
        "    super(MyModel, self).__init__()\n",
        "    # self.conv1 = nn.Conv1d()\n",
        "    self.lstm1 = nn.LSTM(in_utter, hidden_size, bidirectional=True, num_layers=4, dropout=0.5)\n",
        "    self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.output = nn.Linear(hidden_size, out_phone)\n",
        "  \n",
        "  def init_weights(self):\n",
        "    with torch.no_grad():\n",
        "      xavier_init(self.modules())\n",
        "\n",
        "  def forward(self, X, lengths):\n",
        "    packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
        "    packed_out = self.lstm1(packed_X)[0]\n",
        "    out, out_lens = pad_packed_sequence(packed_out)\n",
        "    out = self.fc(out)\n",
        "    out = self.output(out).log_softmax(2)\n",
        "    return out, out_lens\n",
        "\n",
        "\n",
        "  def save(self, ckpt_path):\n",
        "    ckpt = {\n",
        "        'params': self.state_dict()\n",
        "    }\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "  \n",
        "  def load(self, ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    self.load_state_dict(ckpt['params'], strict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UfBk-rCe6T0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd17e4c-5e92-4f1e-d18a-3c5856a3fb33"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = MyModel(13, 42, 512)\n",
        "model.init_weights()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.CTCLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.85)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1,verbose=True, patience=2, threshold=5e-2)\n",
        "\n",
        "def output_2_string(out):\n",
        "  strings = \"\"\n",
        "  for j in range(len(out)):\n",
        "    strings += PHONEME_MAP[int(out[j])]\n",
        "  return strings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "MyModel(\n",
            "  (lstm1): LSTM(13, 512, num_layers=4, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (output): Linear(in_features=512, out_features=42, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u5VIxZ-TsoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092f2f73-3514-496c-ffd9-e0d18582d103"
      },
      "source": [
        "temp = torch.load(\"/content/drive/My Drive/HW3P2 Model_19\")\n",
        "model.load_state_dict(temp['model_state_dict'])\n",
        "\n",
        "optimizer.load_state_dict(temp['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(temp['scheduler_state_dict'])\n",
        "print(scheduler.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'factor': 0.1, 'min_lrs': [0], 'patience': 2, 'verbose': True, 'cooldown': 0, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.05, 'threshold_mode': 'rel', 'best': 7.831046312178388, 'num_bad_epochs': 1, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 16, '_last_lr': [1e-07]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHRtDxfrTXC"
      },
      "source": [
        "def validate(model, data_loader):\n",
        "  model.eval()\n",
        "  test_loss = []\n",
        "  dist = 0\n",
        "  total = 0\n",
        "  for batch_num, data in enumerate(data_loader):\n",
        "    X_pad, X_lens, Y_pad, Y_lens = data\n",
        "    X_pad, Y_pad = X_pad.to(device), Y_pad.to(device) \n",
        "    out, out_lens = model(X_pad, X_lens)\n",
        "    output, _, _, out_seq_len = decoder.decode(out.transpose(0,1), out_lens)\n",
        "\n",
        "    for i in range(len(output)):\n",
        "      string1 = output_2_string(output[i,0,:out_seq_len[i,0]])\n",
        "      string2 = output_2_string(Y_pad[i,:Y_lens[i]])\n",
        "      dist += ls.distance(string1, string2)\n",
        "    loss = criterion(out, Y_pad, out_lens, Y_lens)\n",
        "    total += len(Y_pad)\n",
        "    test_loss.extend([loss.item()]*Y_pad.size()[0])\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del X_pad\n",
        "    del X_lens\n",
        "    del Y_pad\n",
        "    del Y_lens\n",
        "\n",
        "  model.train()\n",
        "  return np.mean(test_loss), dist/total\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxgA0DsIGRsJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cffe2ecc-3ef8-4908-d3ad-af545572d652"
      },
      "source": [
        "model.train()\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(20):\n",
        "  start_time = time.time()\n",
        "  avg_loss = 0.0\n",
        "  for batch_num, data in enumerate(train_loader):\n",
        "    X_pad, X_lens, Y_pad, Y_lens = data\n",
        "    X_pad, Y_pad = X_pad.to(device), Y_pad.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    out, out_lens = model(X_pad, X_lens)\n",
        "    loss = criterion(out, Y_pad, out_lens, Y_lens)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    avg_loss += loss.item()\n",
        "\n",
        "    if batch_num % 50 == 49:\n",
        "        print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "        end_time = time.time()\n",
        "        print(f\"50 batches took {end_time - start_time} seconds\")\n",
        "        avg_loss = 0.0    \n",
        "        start_time = time.time()\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del loss\n",
        "    del X_pad\n",
        "    del Y_pad\n",
        "    del X_lens\n",
        "    del Y_lens\n",
        "  torch.save({'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict' : scheduler.state_dict(),\n",
        "  }, \"/content/drive/My Drive/\"+\"HW3P2 Model_\"+str(epoch+15))\n",
        "\n",
        "  val_loss, val_dist = validate(model, val_loader)\n",
        "  print('Val Loss: {:.4f}\\tVal Distance: {:.4f}'.\n",
        "        format(val_loss, val_dist))\n",
        "  scheduler.step(val_dist)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\tBatch: 50\tAvg-Loss: 0.2400\n",
            "50 batches took 248.69465255737305 seconds\n",
            "Epoch: 1\tBatch: 100\tAvg-Loss: 0.2400\n",
            "50 batches took 252.19888019561768 seconds\n",
            "Epoch: 1\tBatch: 150\tAvg-Loss: 0.2353\n",
            "50 batches took 251.2945535182953 seconds\n",
            "Epoch: 1\tBatch: 200\tAvg-Loss: 0.2404\n",
            "50 batches took 250.70153617858887 seconds\n",
            "Epoch: 1\tBatch: 250\tAvg-Loss: 0.2356\n",
            "50 batches took 248.22054052352905 seconds\n",
            "Epoch: 1\tBatch: 300\tAvg-Loss: 0.2338\n",
            "50 batches took 250.92295384407043 seconds\n",
            "Val Loss: 0.3974\tVal Distance: 7.7166\n",
            "Epoch: 2\tBatch: 50\tAvg-Loss: 0.2361\n",
            "50 batches took 250.09404921531677 seconds\n",
            "Epoch: 2\tBatch: 100\tAvg-Loss: 0.2409\n",
            "50 batches took 251.4939103126526 seconds\n",
            "Epoch: 2\tBatch: 150\tAvg-Loss: 0.2348\n",
            "50 batches took 250.7012550830841 seconds\n",
            "Epoch: 2\tBatch: 200\tAvg-Loss: 0.2370\n",
            "50 batches took 251.26714944839478 seconds\n",
            "Epoch: 2\tBatch: 250\tAvg-Loss: 0.2396\n",
            "50 batches took 250.14151692390442 seconds\n",
            "Epoch: 2\tBatch: 300\tAvg-Loss: 0.2368\n",
            "50 batches took 250.66490077972412 seconds\n",
            "Val Loss: 0.3972\tVal Distance: 7.7148\n",
            "Epoch: 3\tBatch: 50\tAvg-Loss: 0.2346\n",
            "50 batches took 249.59730696678162 seconds\n",
            "Epoch: 3\tBatch: 100\tAvg-Loss: 0.2418\n",
            "50 batches took 252.21783876419067 seconds\n",
            "Epoch: 3\tBatch: 150\tAvg-Loss: 0.2370\n",
            "50 batches took 250.0094757080078 seconds\n",
            "Epoch: 3\tBatch: 200\tAvg-Loss: 0.2375\n",
            "50 batches took 250.2792043685913 seconds\n",
            "Epoch: 3\tBatch: 250\tAvg-Loss: 0.2344\n",
            "50 batches took 249.70826530456543 seconds\n",
            "Epoch: 3\tBatch: 300\tAvg-Loss: 0.2384\n",
            "50 batches took 251.14589858055115 seconds\n",
            "Val Loss: 0.3971\tVal Distance: 7.7114\n",
            "Epoch    15: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 4\tBatch: 50\tAvg-Loss: 0.2405\n",
            "50 batches took 250.80717658996582 seconds\n",
            "Epoch: 4\tBatch: 100\tAvg-Loss: 0.2356\n",
            "50 batches took 251.0809805393219 seconds\n",
            "Epoch: 4\tBatch: 150\tAvg-Loss: 0.2365\n",
            "50 batches took 249.30503273010254 seconds\n",
            "Epoch: 4\tBatch: 200\tAvg-Loss: 0.2384\n",
            "50 batches took 250.64933919906616 seconds\n",
            "Epoch: 4\tBatch: 250\tAvg-Loss: 0.2354\n",
            "50 batches took 250.64765763282776 seconds\n",
            "Epoch: 4\tBatch: 300\tAvg-Loss: 0.2374\n",
            "50 batches took 250.4505696296692 seconds\n",
            "Val Loss: 0.3971\tVal Distance: 7.7088\n",
            "Epoch: 5\tBatch: 50\tAvg-Loss: 0.2355\n",
            "50 batches took 251.18553614616394 seconds\n",
            "Epoch: 5\tBatch: 100\tAvg-Loss: 0.2343\n",
            "50 batches took 250.69734120368958 seconds\n",
            "Epoch: 5\tBatch: 150\tAvg-Loss: 0.2364\n",
            "50 batches took 248.2624671459198 seconds\n",
            "Epoch: 5\tBatch: 200\tAvg-Loss: 0.2375\n",
            "50 batches took 250.99919295310974 seconds\n",
            "Epoch: 5\tBatch: 250\tAvg-Loss: 0.2354\n",
            "50 batches took 251.65144062042236 seconds\n",
            "Epoch: 5\tBatch: 300\tAvg-Loss: 0.2410\n",
            "50 batches took 250.50272583961487 seconds\n",
            "Val Loss: 0.3971\tVal Distance: 7.7071\n",
            "Epoch: 6\tBatch: 50\tAvg-Loss: 0.2389\n",
            "50 batches took 251.26764798164368 seconds\n",
            "Epoch: 6\tBatch: 100\tAvg-Loss: 0.2367\n",
            "50 batches took 251.21949172019958 seconds\n",
            "Epoch: 6\tBatch: 150\tAvg-Loss: 0.2419\n",
            "50 batches took 250.27443146705627 seconds\n",
            "Epoch: 6\tBatch: 200\tAvg-Loss: 0.2344\n",
            "50 batches took 251.00734663009644 seconds\n",
            "Epoch: 6\tBatch: 250\tAvg-Loss: 0.2346\n",
            "50 batches took 248.6735999584198 seconds\n",
            "Epoch: 6\tBatch: 300\tAvg-Loss: 0.2318\n",
            "50 batches took 250.18566226959229 seconds\n",
            "Val Loss: 0.3971\tVal Distance: 7.7105\n",
            "Epoch    18: reducing learning rate of group 0 to 1.0000e-08.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-afb8ce68f41c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-h2yR1fl-BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f61f9d5-2cd9-4685-d0b2-9a374a3da754"
      },
      "source": [
        "test_features = np.load('test.npy', allow_pickle=True, encoding='latin1')\n",
        "print(\"Test length is \", len(test_features))\n",
        "class TestDataset(Dataset):\n",
        "  def __init__(self, X):\n",
        "    self.X = X\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.X[index]\n",
        "    X_len = X.shape[0]\n",
        "    return torch.from_numpy(X).float(), X_len\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "  X, X_len = zip(*batch)\n",
        "  X_lens = torch.LongTensor(X_len)\n",
        "  X_pad = pad_sequence(X)\n",
        "  return X_pad, X_lens\n",
        "\n",
        "test_dataset = TestDataset(test_features)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_of_workers, collate_fn=test_collate_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test length is  2251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBJfn2kNudQp"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model.eval()\n",
        "pred = []\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(test_loader):\n",
        "    X, X_lens = data\n",
        "    X = X.to(device)\n",
        "    out, out_lens = model(X, X_lens)\n",
        "    # print(\"out shape is \",out.shape)\n",
        "    # print(\"out lens is \",out_lens)\n",
        "    output, _, _, out_seq_len = decoder.decode(out.transpose(0,1), out_lens)\n",
        "    for j in range(len(output)):\n",
        "      # if j == 0:\n",
        "      #   print(\"output is like this \", output[j,0,:out_seq_len[j,0]])\n",
        "      pred.append(output[j,0,:out_seq_len[j,0]])\n",
        "    torch.cuda.empty_cache()\n",
        "    del X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYLtIqEIxwdm"
      },
      "source": [
        "out = []\n",
        "for i in pred:\n",
        "  temp = \"\"\n",
        "  for j in range(len(i)):\n",
        "    temp += PHONEME_MAP[i[j]]\n",
        "  out.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFNw9k3S0w1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0973bcf2-c9b3-41b9-b4c6-512b6e18ad50"
      },
      "source": [
        "print(len(out))\n",
        "df = pd.DataFrame(out, columns=['label'])\n",
        "df.to_csv(\"/content/submission.csv\", index_label=\"id\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2251\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}